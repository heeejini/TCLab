{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 경로 설정\n",
    "csv_folders = [\n",
    "    Path(\"C:/Users/Developer/TCLab/csv/data_back3\"),\n",
    "    Path(\"C:/Users/Developer/TCLab/csv/data_back4\"),\n",
    "    Path(\"C:/Users/Developer/TCLab/csv/data_back5\"),\n",
    "    Path(\"C:/Users/Developer/TCLab/csv/data_back6\"),\n",
    "]\n",
    "\n",
    "all_observations = []\n",
    "all_actions = []\n",
    "all_next_observations = []\n",
    "raw_rewards = []\n",
    "all_dones = []\n",
    "\n",
    "alpha = 0.05  # 시간 패널티 계수\n",
    "dt = 5.0      # 샘플 간격 (초)\n",
    "\n",
    "for folder in csv_folders:\n",
    "    csv_files = sorted(folder.glob(\"mpc_episode_*_data.csv\"))\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        for i in range(len(df) - 1):\n",
    "            curr = df.iloc[i]\n",
    "            next_ = df.iloc[i + 1]\n",
    "\n",
    "            state = [curr[\"T1\"], curr[\"T2\"], curr[\"TSP1\"], curr[\"TSP2\"]]\n",
    "            action = [curr[\"Q1\"], curr[\"Q2\"]]\n",
    "            next_state = [next_[\"T1\"], next_[\"T2\"], next_[\"TSP1\"], next_[\"TSP2\"]]\n",
    "\n",
    "            # 에러 및 시간\n",
    "            # error1 = next_[\"TSP1\"] - next_[\"T1\"]\n",
    "            # error2 = next_[\"TSP2\"] - next_[\"T2\"]\n",
    "            # time_sec = i * dt\n",
    "\n",
    "            # # 시간 기반 리워드\n",
    "            # reward = -np.sqrt(error1**2 + error2**2) - alpha * time_sec\n",
    "            error1 = curr[\"TSP1\"] - curr[\"T1\"]\n",
    "            error2 = curr[\"TSP2\"] - curr[\"T2\"]\n",
    "            reward = - np.sqrt(error1**2 + error2 ** 2)\n",
    "            #reward = -np.sqrt(error1**2+ error2**2) \n",
    "            raw_rewards.append(reward)\n",
    "\n",
    "            done = (i == len(df) - 2)\n",
    "            \n",
    "\n",
    "            all_observations.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_next_observations.append(next_state)\n",
    "            all_dones.append(done)\n",
    "\n",
    "# ⭐️ 리워드 표준화\n",
    "scaler = StandardScaler()\n",
    "scaled_rewards = scaler.fit_transform(np.array(raw_rewards).reshape(-1, 1)).flatten()\n",
    "import joblib\n",
    "joblib.dump(scaler, \"first_reward.pkl\")\n",
    "# 데이터셋 저장\n",
    "dataset = {\n",
    "    \"observations\": np.array(all_observations, dtype=np.float32),\n",
    "    \"actions\": np.array(all_actions, dtype=np.float32),\n",
    "    \"next_observations\": np.array(all_next_observations, dtype=np.float32),\n",
    "    \"rewards\": scaled_rewards.astype(np.float32),\n",
    "    \"terminals\": np.array(all_dones, dtype=bool),\n",
    "}\n",
    "# 추가 디버깅 출력\n",
    "acts = dataset['actions']\n",
    "print(\"[Debug] Action range in dataset:\", acts.min(), \"~\", acts.max())\n",
    "print(\"[Debug] Action mean:\", acts.mean(axis=0), \"std:\", acts.std(axis=0))\n",
    "\n",
    "rewards = scaled_rewards['rewards']\n",
    "print(\"[Debug] rewards range in dataset:\", scaled_rewards.min(), \"~\", scaled_rewards.max())\n",
    "print(\"[Debug] rewards mean:\", scaled_rewards.mean(axis=0), \"std:\", scaled_rewards.std(axis=0))\n",
    "\n",
    "observations = dataset['observations']\n",
    "print(\"[Debug] observations range in dataset:\", observations.min(), \"~\", observations.max())\n",
    "print(\"[Debug] observations mean:\", observations.mean(axis=0), \"std:\", observations.std(axis=0))\n",
    "# output_path = \"C:/Users/Developer/TCLab/Data/MPC/first_reward.npz\"\n",
    "# np.savez(output_path, **dataset)\n",
    "# print(f\"✅ 저장 완료: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
