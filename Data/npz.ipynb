{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 평균: -9.310084092646807\n",
      "Reward 최소: -47.412083902735176\n",
      "Reward 최대: -0.00999999999999801\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 설정C:\\Users\\Developer\\TCLab\\csv\n",
    "csv_folder = Path(\"C:\\\\Users\\\\Developer\\\\TCLab\\\\csv\")\n",
    "\n",
    "csv_files = sorted(csv_folder.glob(\"mpc_episode_*_data.csv\"))\n",
    "# C:\\Users\\Developer\\TCLab\\csv\\data_back4\n",
    "E1, E2 = 1.0, 1.0  # 가중치 설정\n",
    "\n",
    "all_observations = []\n",
    "all_actions = []\n",
    "all_next_observations = []\n",
    "all_rewards = []\n",
    "all_dones = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    for i in range(len(df) - 1):\n",
    "        curr = df.iloc[i]\n",
    "        next_ = df.iloc[i + 1]\n",
    "        \n",
    "        state = [curr[\"T1\"], curr[\"T2\"], curr[\"TSP1\"], curr[\"TSP2\"]]\n",
    "        action = [curr[\"Q1\"], curr[\"Q2\"]]\n",
    "        next_state = [next_[\"T1\"], next_[\"T2\"], next_[\"TSP1\"], next_[\"TSP2\"]]\n",
    "        \n",
    "        # L2 norm 기반 reward\n",
    "        error_vec = np.array([curr[\"TSP1\"] - curr[\"T1\"], curr[\"TSP2\"] - curr[\"T2\"]])\n",
    "        reward = -np.linalg.norm(error_vec)\n",
    "        \n",
    "        done = (i == len(df) - 2)\n",
    "\n",
    "        all_observations.append(state)\n",
    "        all_actions.append(action)\n",
    "        all_next_observations.append(next_state)\n",
    "        all_rewards.append(reward)\n",
    "        all_dones.append(done)\n",
    "\n",
    "# numpy 배열로 변환\n",
    "dataset = {\n",
    "    \"observations\": np.array(all_observations, dtype=np.float32),\n",
    "    \"actions\": np.array(all_actions, dtype=np.float32),\n",
    "    \"next_observations\": np.array(all_next_observations, dtype=np.float32),\n",
    "    \"rewards\": np.array(all_rewards, dtype=np.float32),\n",
    "    \"terminals\": np.array(all_dones, dtype=bool),\n",
    "}\n",
    "print(\"Reward 평균:\", np.mean(all_rewards))\n",
    "print(\"Reward 최소:\", np.min(all_rewards))\n",
    "print(\"Reward 최대:\", np.max(all_rewards))\n",
    "\n",
    "# 저장\n",
    "output_path_l2 = csv_folder.parent / \"mpc_dataset.npz\"\n",
    "np.savez(output_path_l2, **dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e1062",
   "metadata": {},
   "source": [
    "reward = -np.linalg.norm(error_vec) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f51ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 평균: -0.931008409264681\n",
      "Reward 최소: -4.741208390273518\n",
      "Reward 최대: -0.0009999999999998012\n",
      "[Debug] Action range in dataset: 0.0 ~ 100.0\n",
      "[Debug] Action mean: [30.623505 40.10309 ] std: [42.8137   44.337543]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 설정C:\\Users\\Developer\\TCLab\\csv\n",
    "csv_folder = Path(\"C:\\\\Users\\\\Developer\\\\TCLab\\\\csv\")\n",
    "csv_files = sorted(csv_folder.glob(\"mpc_episode_*_data.csv\"))\n",
    "\n",
    "E1, E2 = 1.0, 1.0  # 가중치 설정\n",
    "\n",
    "all_observations = []\n",
    "all_actions = []\n",
    "all_next_observations = []\n",
    "all_rewards = []\n",
    "all_dones = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    for i in range(len(df) - 1):\n",
    "        curr = df.iloc[i]\n",
    "        next_ = df.iloc[i + 1]\n",
    "        \n",
    "        state = [curr[\"T1\"], curr[\"T2\"], curr[\"TSP1\"], curr[\"TSP2\"]]\n",
    "        action = [curr[\"Q1\"], curr[\"Q2\"]]\n",
    "        next_state = [next_[\"T1\"], next_[\"T2\"], next_[\"TSP1\"], next_[\"TSP2\"]]\n",
    "        \n",
    "        # L2 norm 기반 reward\n",
    "        error_vec = np.array([curr[\"TSP1\"] - curr[\"T1\"], curr[\"TSP2\"] - curr[\"T2\"]])\n",
    "        reward = -np.linalg.norm(error_vec) * 0.1\n",
    "        \n",
    "        done = (i == len(df) - 2)\n",
    "\n",
    "        all_observations.append(state)\n",
    "        all_actions.append(action)\n",
    "        all_next_observations.append(next_state)\n",
    "        all_rewards.append(reward)\n",
    "        all_dones.append(done)\n",
    "\n",
    "# numpy 배열로 변환\n",
    "dataset = {\n",
    "    \"observations\": np.array(all_observations, dtype=np.float32),\n",
    "    \"actions\": np.array(all_actions, dtype=np.float32),\n",
    "    \"next_observations\": np.array(all_next_observations, dtype=np.float32),\n",
    "    \"rewards\": np.array(all_rewards, dtype=np.float32),\n",
    "    \"terminals\": np.array(all_dones, dtype=bool),\n",
    "}\n",
    "print(\"Reward 평균:\", np.mean(all_rewards))\n",
    "print(\"Reward 최소:\", np.min(all_rewards))\n",
    "print(\"Reward 최대:\", np.max(all_rewards))\n",
    "\n",
    "acts = dataset['actions']\n",
    "\n",
    "print(\"[Debug] Action range in dataset:\", acts.min(), \"~\", acts.max())\n",
    "print(\"[Debug] Action mean:\", acts.mean(axis=0), \"std:\", acts.std(axis=0))\n",
    "\n",
    "# 저장\n",
    "# output_path_l2 = csv_folder.parent / \"mpc_dataset1.npz\"\n",
    "# np.savez(output_path_l2, **dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b042a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 평균: 0.20997810061982278\n",
      "Reward 최소: -3.741208390273518\n",
      "Reward 최대: 0.9990000000000002\n",
      "[Debug] Action range in dataset: 0.0 ~ 100.0\n",
      "[Debug] Action mean: [26.522545 40.997356] std: [40.481014 43.941162]\n",
      "[Debug] rewards range in dataset: -3.7412083 ~ 0.999\n",
      "[Debug] rewards mean: 0.20997809 std: 0.86896694\n",
      "[Debug] observations range in dataset: 25.22 ~ 68.24\n",
      "[Debug] observations mean: [44.730286 43.0406   44.32769  43.805374] std: [10.942937  9.281088 11.969531 10.855006]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 두 개의 csv 디렉토리 경로\n",
    "csv_folders = [\n",
    "    Path(\"C:/Users/Developer/TCLab/csv\"),\n",
    "    Path(\"C:/Users/Developer/TCLab/csv/data_back4\")\n",
    "]\n",
    "\n",
    "E1, E2 = 1.0, 1.0  # 가중치 설정\n",
    "\n",
    "all_observations = []\n",
    "all_actions = []\n",
    "all_next_observations = []\n",
    "all_rewards = []\n",
    "all_dones = []\n",
    "\n",
    "# 모든 폴더에서 CSV 병합\n",
    "for folder in csv_folders:\n",
    "    csv_files = sorted(folder.glob(\"mpc_episode_*_data.csv\"))\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        for i in range(len(df) - 1):\n",
    "            curr = df.iloc[i]\n",
    "            next_ = df.iloc[i + 1]\n",
    "            \n",
    "            state = [curr[\"T1\"], curr[\"T2\"], curr[\"TSP1\"], curr[\"TSP2\"]]\n",
    "            action = [curr[\"Q1\"], curr[\"Q2\"]]\n",
    "            next_state = [next_[\"T1\"], next_[\"T2\"], next_[\"TSP1\"], next_[\"TSP2\"]]\n",
    "            \n",
    "            # L2 norm 기반 reward\n",
    "            error_vec = np.array([curr[\"TSP1\"] - curr[\"T1\"], curr[\"TSP2\"] - curr[\"T2\"]])\n",
    "            reward = -np.linalg.norm(error_vec)\n",
    "            reward += 10.0  # shift: 음수를 양수로\n",
    "            reward *= 0.1   # scale: 보상 범위 조정\n",
    "\n",
    "          #  reward += 10.0    # 적절한 offset\n",
    "          #  reward *=     \n",
    "            \n",
    "            done = (i == len(df) - 2)\n",
    "\n",
    "            all_observations.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_next_observations.append(next_state)\n",
    "            all_rewards.append(reward)\n",
    "            all_dones.append(done)\n",
    "\n",
    "# numpy 배열로 변환\n",
    "dataset = {\n",
    "    \"observations\": np.array(all_observations, dtype=np.float32),\n",
    "    \"actions\": np.array(all_actions, dtype=np.float32),\n",
    "    \"next_observations\": np.array(all_next_observations, dtype=np.float32),\n",
    "    \"rewards\": np.array(all_rewards, dtype=np.float32),\n",
    "    \"terminals\": np.array(all_dones, dtype=bool),\n",
    "}\n",
    "\n",
    "# 리워드 통계 출력\n",
    "print(\"Reward 평균:\", np.mean(all_rewards))\n",
    "print(\"Reward 최소:\", np.min(all_rewards))\n",
    "print(\"Reward 최대:\", np.max(all_rewards))\n",
    "\n",
    "acts = dataset['actions']\n",
    "\n",
    "print(\"[Debug] Action range in dataset:\", acts.min(), \"~\", acts.max())\n",
    "print(\"[Debug] Action mean:\", acts.mean(axis=0), \"std:\", acts.std(axis=0))\n",
    "\n",
    "rewards = dataset['rewards']\n",
    "print(\"[Debug] rewards range in dataset:\", rewards.min(), \"~\", rewards.max())\n",
    "print(\"[Debug] rewards mean:\", rewards.mean(axis=0), \"std:\", rewards.std(axis=0))\n",
    "\n",
    "observations = dataset['observations']\n",
    "print(\"[Debug] observations range in dataset:\", observations.min(), \"~\", observations.max())\n",
    "print(\"[Debug] observations mean:\", observations.mean(axis=0), \"std:\", observations.std(axis=0))\n",
    "# 저장 경로 (부모 디렉토리에 저장)\n",
    "output_path = csv_folders[0].parent / \"mpc_dataset_merged.npz\"\n",
    "np.savez(output_path, **dataset)\n",
    "print(f\"✅ 저장 완료: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
