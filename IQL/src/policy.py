import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal
import random 

from .util import mlp


LOG_STD_MIN = -5.0
LOG_STD_MAX = 2.0

# stochastic policy function and deterministic policy function 

class GaussianPolicy(nn.Module):
    # í™•ë¥ ì ì •ì±… 
    def __init__(self, obs_dim, act_dim, hidden_dim=256, n_hidden=2):
        super().__init__()
        self.net = mlp([obs_dim, *([hidden_dim] * n_hidden), act_dim])
        self.log_std = nn.Parameter(torch.zeros(act_dim, dtype=torch.float32))
        self.epsilon = 1 # ì´ˆê¸° ê°’ 

    def forward(self, obs):\
        # ìƒíƒœ obs ë¥¼ ë°›ì•„ì„œ í‰ê·  mean ê³¼ ê³µë¶„ì‚° scale_tril ë¡œ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ë¥¼ ìƒì„±í•¨ 
        mean = self.net(obs)
        std = torch.exp(self.log_std.clamp(LOG_STD_MIN, LOG_STD_MAX))
        scale_tril = torch.diag(std)
        return MultivariateNormal(mean, scale_tril=scale_tril)
        # if mean.ndim > 1:
        #     batch_size = len(obs)
        #     return MultivariateNormal(mean, scale_tril=scale_tril.repeat(batch_size, 1, 1))
        # else:
        #     return MultivariateNormal(mean, scale_tril=scale_tril)

    def act(self, obs, deterministic=False, enable_grad=False):
        with torch.set_grad_enabled(enable_grad):
            dist = self(obs)
            # exploration ì‹œì—ëŠ” dist.sample() , evaluation ì‹œì—ëŠ” dist.mean() ì‚¬ìš© 
            return dist.mean if deterministic else dist.sample()

# ì•ˆì „ ë²”ìœ„ ë‚´ íƒìƒ‰ 
    def error_act(
        self,
        obs, 
        deterministic: bool = False,
        enable_grad: bool = False,
        err_thr: float = 3,        # |Tspâˆ’T| â‰¤ err_thr ì¼ ë•Œë§Œ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise_std: float = 5.0       # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨ (PWM %)
    ):
        """
        1 + 4 íƒìƒ‰ ì „ëµ:
        - í° ì˜¤ì°¨(|Tsp - T| > err_thr) â†’ í•™ìŠµëœ policyì˜ í‰ê· (mean)ë§Œ ì‚¬ìš©
        - ì‘ì€ ì˜¤ì°¨(|Tsp - T| â‰¤ err_thr) â†’ mean + N(0, noise_stdÂ²) ë¡œ íƒìƒ‰
        """
        with torch.set_grad_enabled(enable_grad):
            dist = self(obs)
            print("obs shape:", obs.shape)
            mean_action = dist.mean                      # [Q1_mean, Q2_mean]

            if deterministic:
                # í‰ê°€ ëª¨ë“œ: í•­ìƒ í‰ê· 
                return torch.clamp(mean_action, 0.0, 100.0)

            # â”€â”€ í˜„ì¬ ì˜¤ì°¨ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            delta1 = obs[2] - obs[0]   # TSP1 - T1
            delta2 = obs[3] - obs[1]   # TSP2 - T2

            in_safe_region = (
                torch.abs(delta1) <= err_thr and
                torch.abs(delta2) <= err_thr
            )
            if in_safe_region:
                std = torch.full_like(mean_action, noise_std)
                noise = torch.normal(mean=torch.zeros_like(mean_action), std=std)
                action = mean_action + noise
                print(f"íƒìƒ‰ (noise added) : err1= {delta1:.2f}, err2= {delta2 :.2f}")
            else:
                action = mean_action
              #  print(f"ë³´ìˆ˜ì  í–‰ë™: err1={delta1:.2f}, err2={delta2:.2f}")

            return torch.clamp(action, 0.0, 100.0)
    def directional_override_act(
        self,
        obs: torch.Tensor,
        deterministic: bool = False,
        enable_grad: bool = False,
        err_thr: float = 1.0
    ):
        """
        Îµ-greedy + directional override ê¸°ë°˜ íƒìƒ‰ ì •ì±…
        """
        with torch.set_grad_enabled(enable_grad):
            dist = self(obs)
            mean_action = dist.mean  # [Q1, Q2]

            if deterministic:
                return torch.clamp(mean_action, 0.0, 100.0)

            if random.random() < self.epsilon:
                # ğŸ”¥ directional override ì‚¬ìš©
                print("ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ ì…ì‹¤ë¡  ê·¸ë¦¬ë””!!! ")
                delta1 = obs[2] - obs[0]
                delta2 = obs[3] - obs[1]

                if delta1 > err_thr:
                    Q1 = 100.0
                    print(f"ğŸ”¥ Q1 = 100 (delta1 = {delta1.item():.2f} > {err_thr})")
                elif delta1 < -err_thr:
                    Q1 = 0.0
                    print(f"â„ï¸ Q1 = 0 (delta1 = {delta1.item():.2f} < -{err_thr})")
                else:
                    Q1 = mean_action[0]
                    print(f"âœ… Q1 = mean ({Q1.item():.2f}) (|delta1| <= {err_thr})")

                if delta2 > err_thr:
                    Q2 = 100.0
                    print(f"ğŸ”¥ Q2 = 100 (delta2 = {delta2.item():.2f} > {err_thr})")
                elif delta2 < -err_thr:
                    Q2 = 0.0
                    print(f"â„ï¸ Q2 = 0 (delta2 = {delta2.item():.2f} < -{err_thr})")
                else:
                    Q2 = mean_action[1]
                    print(f"âœ… Q2 = mean ({Q2.item():.2f}) (|delta2| <= {err_thr})")
            else:
                # âœ… Îµë¥¼ ë„˜ê¸´ ê²½ìš°ëŠ” ê·¸ëƒ¥ í‰ê·  ì •ì±… ì‚¬ìš©
                Q1, Q2 = mean_action[0], mean_action[1]
                print(f"ğŸ¯ Q = policy mean ({Q1.item():.2f}, {Q2.item():.2f})")

            action = torch.tensor([Q1, Q2], device=obs.device)
            return torch.clamp(action, 0.0, 100.0)

    def guided_act(
            self,
            obs: torch.Tensor,
            deterministic: bool = False,
            enable_grad: bool = False,
            err_thr: float = 1.0,
            max_noise_std: float = 10.0,
            bias_scale: float = 0.5
        ):
        """
        TSPì— ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ê¸° ìœ„í•œ guided exploration í•¨ìˆ˜.

        - í° ì˜¤ì°¨ì¼ìˆ˜ë¡ ë” í° íƒìƒ‰(std) + ì˜¤ì°¨ ë°©í–¥ìœ¼ë¡œ bias ì¶”ê°€
        - ì‘ì€ ì˜¤ì°¨ëŠ” í•™ìŠµëœ policyì˜ í‰ê· (mean)ë§Œ ì‚¬ìš©
        """
        with torch.set_grad_enabled(enable_grad):
            dist = self(obs)
            mean_action = dist.mean  # [Q1, Q2]

            if deterministic:
                return torch.clamp(mean_action, 0.0, 100.0)

            # ì˜¤ì°¨ ê³„ì‚°
            delta1 = obs[2] - obs[0]  # TSP1 - T1
            delta2 = obs[3] - obs[1]  # TSP2 - T2
            err_norm = torch.sqrt(delta1**2 + delta2**2)

            # ë…¸ì´ì¦ˆ stdëŠ” ì˜¤ì°¨ì— ë¹„ë¡€
            scaled_std = min(err_norm.item() * 5.0, max_noise_std)
            noise = torch.normal(
                mean=torch.zeros_like(mean_action),
                std=torch.full_like(mean_action, scaled_std)
            )

            # ì˜¤ì°¨ ë°©í–¥ìœ¼ë¡œ bias ì¶”ê°€
            bias = torch.tensor([
                bias_scale * delta1.item(),
                bias_scale * delta2.item()
            ], device=obs.device)

            # ìµœì¢… í–‰ë™ = mean + bias + noise
            action = mean_action + bias + noise
            print(action[0], action[1])
            return torch.clamp(action, 0.0, 100.0)

    # def act(self, obs, deterministic=False, enable_grad=False, exploration_std=0.03):
    #     """
    #     IQL ë…¼ë¬¸ ìŠ¤íƒ€ì¼ì˜ íƒìƒ‰:
    #     - deterministic: í‰ê· ë§Œ ì‚¬ìš©
    #     - exploration: mean + N(0, ÏƒÂ²) (Ïƒ=0.03 ì¶”ì²œ)
    #     """
    #     with torch.set_grad_enabled(enable_grad):
    #         dist = self(obs)
    #         mean = dist.mean

    #         if deterministic:
    #             return torch.clamp(mean, 0.0, 100.0)  # ì•ˆì „ ë²”ìœ„ ë‚´ clip (ì„ íƒ)

    #         # IQL ë…¼ë¬¸ì‹ exploration: mean + Gaussian noise
    #         noise = torch.normal(
    #             mean=torch.zeros_like(mean),
    #             std=exploration_std
    #         )
    #         action = mean + noise
    #         return torch.clamp(action, 0.0, 100.0)  # clamp ì—†ìœ¼ë©´ íˆí„°ê°€ ìŒìˆ˜ì¼ ìˆ˜ë„ ìˆìŒ

    #def online_act (self, obs, deterministic=False, enable_grad = False, )
    def online_act(
        self,
        obs: torch.Tensor,
        deterministic: bool = False,
        enable_grad: bool = False,
        err_thr: float = 4.0,        # |Tspâˆ’T| â‰¤ err_thr ì¼ ë•Œë§Œ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise_std: float = 10.0       # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨ (PWM %)
    ):
        """
        1 + 4 íƒìƒ‰ ì „ëµ:
        - í° ì˜¤ì°¨(|Tsp - T| > err_thr) â†’ í•™ìŠµëœ policyì˜ í‰ê· (mean)ë§Œ ì‚¬ìš©
        - ì‘ì€ ì˜¤ì°¨(|Tsp - T| â‰¤ err_thr) â†’ mean + N(0, noise_stdÂ²) ë¡œ íƒìƒ‰
        """
        with torch.set_grad_enabled(enable_grad):
            dist = self(obs)
            mean_action = dist.mean                      # [Q1_mean, Q2_mean]

            if deterministic:
                # í‰ê°€ ëª¨ë“œ: í•­ìƒ í‰ê· 
                return torch.clamp(mean_action, 0.0, 100.0)

            # â”€â”€ í˜„ì¬ ì˜¤ì°¨ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            delta1 = obs[2] - obs[0]   # TSP1 - T1
            delta2 = obs[3] - obs[1]   # TSP2 - T2

            in_safe_region = (
                torch.abs(delta1) <= err_thr and
                torch.abs(delta2) <= err_thr
            )

            if in_safe_region:
                noise = torch.normal(
                    mean=torch.zeros_like(mean_action),
                    std=noise_std,
                    device=obs.device
                )
                action = mean_action + noise
            else:
                action = mean_action

            return torch.clamp(action, 0.0, 100.0)



class DeterministicPolicy(nn.Module):
    # ê²°ì •ë¡ ì  ì •ì±… 
    # ìƒíƒœë¥¼ ë°›ì•„ì„œ ì§ì ‘ í–‰ë™ì„ ì¶œë ¥í•¨ 
    def __init__(self, obs_dim, act_dim, hidden_dim=256, n_hidden=2):
        super().__init__()
        self.net = mlp([obs_dim, *([hidden_dim] * n_hidden), act_dim],
                       output_activation=nn.Tanh)

 #### 
    def forward(self, obs):
        out = self.net(obs)         # âˆˆ [-1, 1]
        return 50.0 * (out + 1.0)   # â†’ âˆˆ [0, 100]
        # return (self.net(obs))

    def act(self, obs, deterministic=False, enable_grad=False):
        with torch.set_grad_enabled(enable_grad):
            return self(obs)