# %%
# Policy evaluation notebook cell
#
# â‘  ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH = r"C:\Users\Developer\TCLab\IQL\logs\mpc-iql\04-23-25_14.45.10_ukeq\final.pt"
EVAL_LOG_ROOT = r"C:\Users\Developer\TCLab\IQL\eval_sim_logs"
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# â‘¡ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, time, csv, math, torch, numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

from tclab import setup                    # BYU TCLab ì‹œë®¬ë ˆì´í„°
# util.py ì— ì´ë¯¸ ì¡´ì¬í•œë‹¤ê³  ê°€ì •
from .util import torchify, set_seed

from .policy import GaussianPolicy

# â‘¢ sim_evaluate_policy í•¨ìˆ˜ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_random_tsp(
    steps: int,
    label: str = "TSP",
    low: float = 25.0,
    high: float = 65.0,
    min_time: float = 160.0,   # â† ê·¸ëŒ€ë¡œ ë‘ë˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤
    dt: float = 1.0,
) -> np.ndarray:
    """
    ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸(MPCDataCollector)ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ
    set-point í”„ë¡œíŒŒì¼ì„ ë§Œë“ ë‹¤.

    - ê° êµ¬ê°„ ê¸¸ì´ ~ ğ’©(480 s, 100Â² sÂ²), ë‹¨ 160 sâ€“800 s ë²”ìœ„ë¡œ clip
    - dt ê°„ê²©ìœ¼ë¡œ steps ë§Œí¼ ì±„ìš¸ ë•Œê¹Œì§€ ë°˜ë³µ
    - ì˜¨ë„ëŠ” [low, high] ê· ë“± ë¶„í¬
    """
    tsp = np.zeros(steps)
    i = 0
    seg_id = 1
    print(f"[{label}] --- Set-point í”„ë¡œíŒŒì¼ ìƒì„± (ì´ step={steps}, dt={dt}s) ---")
    while i < steps:
        # êµ¬ê°„ ê¸¸ì´(ì´ˆ) ìƒ˜í”Œë§ í›„ step ë‹¨ìœ„ë¡œ ë³€í™˜
        dur_sec   = float(np.clip(np.random.normal(480, 100), 160, 800))
        dur_steps = max(1, int(dur_sec / dt))
        end       = min(i + dur_steps, steps)

        # ëª©í‘œ ì˜¨ë„ ì„¤ì •
        temp = round(np.random.uniform(low, high), 2)
        tsp[i:end] = temp

        # ë¡œê·¸ ì¶œë ¥
        start_t = int(i * dt)
        end_t   = int((end - 1) * dt)
        print(
            f"[{label}] êµ¬ê°„ {seg_id}: step {i:>4} ~ {end-1:>4} "
            f"(ì‹œê°„ {start_t:>4}s ~ {end_t:>4}s) â†’ ëª©í‘œ ì˜¨ë„ {temp:.2f}Â°C"
        )

        i = end
        seg_id += 1
    print(f"[{label}] -----------------------------------------------------------\n")
    return tsp


def sim_evaluate_policy(
    policy,
    max_steps=1200,
    log_root="./eval_logs",
    seed=1,
    ambient=29.0,
    deterministic=True,
):
    set_seed(seed)
    run_dir = Path(log_root) / f"sim_seed{seed}"
    run_dir.mkdir(parents=True, exist_ok=True)

    lab = setup(connected=False)
    env = lab(synced=False)
    env.T_amb = 29.0  # ì§ì ‘ ambient ì„¤ì • (ì†ì„±ëª…ì€ T_amb ë˜ëŠ” ambientì¼ ìˆ˜ ìˆìŒ)

    env.Q1(0); env.Q2(0)


    Tsp1 = generate_random_tsp(max_steps, 'TSP1', dt=1.0)
    Tsp2 = generate_random_tsp(max_steps, 'TSP2', dt=1.0)

    t  = np.arange(max_steps)
    T1 = np.zeros(max_steps); T2 = np.zeros(max_steps)
    Q1 = np.zeros(max_steps); Q2 = np.zeros(max_steps)

    total_return = 0.0
    e1 = e2 = over = under = 0.0

    policy.eval()

    for k in range(max_steps):
        env.update(t=k)

        T1[k] = env.T1
        T2[k] = env.T2
        obs = np.array([T1[k], T2[k], Tsp1[k], Tsp2[k]], dtype=np.float32)

        with torch.no_grad():
            act = policy.act(torchify(obs), deterministic=deterministic).cpu().numpy()

        Q1[k] = float(np.clip(act[0], 0, 100))
        Q2[k] = float(np.clip(act[1], 0, 100))
        env.Q1(Q1[k]); env.Q2(Q2[k])

        reward = -math.hypot(T1[k] - Tsp1[k], T2[k] - Tsp2[k])
        total_return += reward

        err1 = Tsp1[k] - T1[k]
        err2 = Tsp2[k] - T2[k]
        e1 += abs(err1);  e2 += abs(err2)
        over  += max(0, -err1) + max(0, -err2)
        under += max(0,  err1) + max(0,  err2)

    env.Q1(0); env.Q2(0)

    # CSV ì €ì¥
    csv_path = run_dir / "rollout.csv"
    with open(csv_path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["time","T1","T2","Q1","Q2","TSP1","TSP2"])
        for k in range(max_steps):
            w.writerow([t[k], T1[k], T2[k], Q1[k], Q2[k], Tsp1[k], Tsp2[k]])

    # ê·¸ë˜í”„ ì €ì¥
    fig, ax = plt.subplots(2,1,figsize=(10,8))
    ax[0].plot(t, T1, label="T1"); ax[0].plot(t, Tsp1, "--", label="TSP1")
    ax[0].plot(t, T2, label="T2"); ax[0].plot(t, Tsp2, ":", label="TSP2")
    ax[0].set_ylabel("Temp (Â°C)"); ax[0].legend(); ax[0].grid()

    ax[1].plot(t, Q1, label="Q1"); ax[1].plot(t, Q2, label="Q2")
    ax[1].set_ylabel("Heater (%)"); ax[1].set_xlabel("Time (s)")
    ax[1].legend(); ax[1].grid()
    plt.tight_layout()
    plt.savefig(run_dir / "rollout.png")
    plt.show()

    metrics = dict(return_sum=total_return, E1=e1, E2=e2, Over=over, Under=under)
    return metrics

# â‘£ ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬ì„± & ê°€ì¤‘ì¹˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
policy_net = GaussianPolicy(obs_dim=4, act_dim=2)
ckpt = torch.load(MODEL_PATH, map_location="cpu")

# 'policy.' ì ‘ë‘ì‚¬ë§Œ ì¶”ì¶œ
subdict = {k.replace("policy.", ""): v for k, v in ckpt.items() if k.startswith("policy.")}
policy_net.load_state_dict(subdict)

# â‘¤ ì‹œë®¬ë ˆì´í„° í‰ê°€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
metrics = sim_evaluate_policy(
    policy=policy_net,
    max_steps=1200,
    log_root=EVAL_LOG_ROOT,
    seed=1,
    ambient=29.0,
    deterministic=True
)

print("Evaluation metrics:", metrics)

