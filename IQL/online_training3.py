import torch
import numpy as np
from pathlib import Path
from tqdm import trange
import joblib
import wandb
import copy 
from pathlib import Path 
from datetime import datetime
import json 
from src.policy import GaussianPolicy, DeterministicPolicy
from src.value_functions import TwinQ, ValueFunction
from src.iql import ImplicitQLearning
from src.util import torchify, Log, set_seed, sample_batch, evaluate_policy_sim, evaluate_policy_tclab
from src.sam import SAM 
from src.eval_policy import compute_reward 
from collections import deque
N_STEP_REWARD = 5

def eval_policy( policy, args):
    if args.type == "simulator":
        return evaluate_policy_sim(policy, args)
    else:
        return evaluate_policy_tclab(policy, args)


def build_optimizer_factory(args):
    if args.sam:
        return lambda params: SAM(params,
                                  torch.optim.Adam,
                                  lr=args.learning_rate,
                                  betas=(0.9, 0.999),
                                  rho=args.sam_rho)
    else:
        return lambda params: torch.optim.Adam(params,
                                               lr=args.learning_rate)
import time
def generate_random_tsp1(
    total_time_sec: int = 1200,
    dt: float = 5.0,  # sample / control interval
    low: float = 25.0,
    high: float = 65.0,
    verbose: bool = False,
) -> np.ndarray:
    """
    Set-point êµ¬ê°„ë³„ ì •ë³´ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥.
    ê° êµ¬ê°„ ê¸¸ì´: í‰ê·  480ì´ˆ, Ïƒ 100ì´ˆ, ìµœì†Œ 160ì´ˆ, ìµœëŒ€ 800ì´ˆ
    """
    # ğŸ² ì‹œë“œ ë¬´ì‘ìœ„ ì´ˆê¸°í™” (ì‹œê°„ + ì¡ìŒ ê¸°ë°˜)
    seed = int((time.time() * 1e6) % 1e9)  # ë§ˆì´í¬ë¡œì´ˆ ê¸°ë°˜ seed
    np.random.seed(seed)
    print(f"[ğŸ§ª TSP ìƒì„±ìš© ì‹œë“œ: {seed}]")

    n_steps = int(total_time_sec / dt)
    tsp = np.zeros(n_steps)
    i = 0
    seg_id = 1  # êµ¬ê°„ ë²ˆí˜¸

    print(f"\n--- Set-point í”„ë¡œíŒŒì¼ ìƒì„± (ì´ ì‹œê°„: {total_time_sec}ì´ˆ, ì´ step: {n_steps}) ---")
    while i < n_steps:
        dur_sec = int(np.clip(np.random.normal(480, 100), 160, 800))
        dur_steps = max(1, int(dur_sec / dt))
        end = min(i + dur_steps, n_steps)

        temp = round(np.random.uniform(low, high), 2)
        tsp[i:end] = temp

        start_time = int(i * dt)
        end_time = int((end - 1) * dt)
        print(f"êµ¬ê°„ {seg_id}: step {i:>3} ~ {end-1:>3} (ì‹œê°„ {start_time:>4}s ~ {end_time:>4}s) â†’ ëª©í‘œ ì˜¨ë„: {temp:.2f}Â°C")

        i = end
        seg_id += 1
    print("-----------------------------------------------------------\n")
    return tsp
from collections import deque           # â˜… ì¶”ê°€

def rollout_tclab(policy, buffer, reward_scaler, args):
    """
      reward_type = 1 :  |TSP_t âˆ’ T_t|
      reward_type = 2 :  |TSP_t âˆ’ T_{t+1}|
      reward_type = 3 :  |TSP_t âˆ’ T_{t+N}|   (N = args.n_step)
    """
    import time, joblib
    from tclab import TCLab
    from tqdm import trange
    from src.util import torchify

    dt    = args.sample_interval
    steps = int(args.max_episode_steps / dt)
    N     = getattr(args, "n_step", 5)      
    ambient = 29.0

    Tsp1 = generate_random_tsp1(args.max_episode_steps, dt)
    Tsp2 = generate_random_tsp1(args.max_episode_steps, dt)

    obs_q, act_q, tsp_q, temp_q = deque(), deque(), deque(), deque()

    with TCLab() as arduino:
        arduino.LED(100)
        arduino.Q1(0); arduino.Q2(0)

        while arduino.T1 > ambient or arduino.T2 > ambient:
            time.sleep(10)

        policy.eval()

        for k in trange(steps, desc="rollout-tclab"):
            loop_start = time.time()

            T1, T2 = arduino.T1, arduino.T2
            obs    = np.array([T1, T2, Tsp1[k], Tsp2[k]], dtype=np.float32)

            with torch.no_grad():
                act = policy.directional_override_act(torchify(obs),
                                 deterministic=args.deterministic_policy).cpu().numpy()
            Q1, Q2 = float(np.clip(act[0], 0, 100)), float(np.clip(act[1], 0, 100))
            arduino.Q1(Q1); arduino.Q2(Q2)

            time.sleep(max(0.0, dt - (time.time() - loop_start)))

            next_T1, next_T2 = arduino.T1, arduino.T2
            next_obs = np.array(
                [next_T1, next_T2,
                 Tsp1[min(k+1, steps-1)], Tsp2[min(k+1, steps-1)]],
                dtype=np.float32
            )

            obs_q.append(obs)
            act_q.append([Q1, Q2])
            tsp_q.append([Tsp1[k], Tsp2[k]])
            temp_q.append([T1, T2])

            if args.reward_type in (1, 2):
                # ê¸°ì¡´ 1-step ë°©ì‹
                if args.reward_type == 1:
                    err1, err2 = Tsp1[k] - T1, Tsp2[k] - T2
                else:  # 2
                    err1, err2 = Tsp1[min(k+1, steps-1)] - next_T1, \
                                 Tsp2[min(k+1, steps-1)] - next_T2
                reward = compute_reward(err1, err2, reward_scaler)
                done   = (k == steps-1)
                buffer.add_transition(obs, [Q1, Q2], next_obs, reward, done)

            elif args.reward_type == 3:
                # n-step: íê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´ pop & ê³„ì‚°
                if len(obs_q) >= N:
                    s_t      = obs_q.popleft()
                    a_t      = act_q.popleft()
                    tsp_now  = tsp_q.popleft() 
                    t_future = temp_q.pop()      # N step ë’¤ ì˜¨ë„

                    err1 = tsp_now[0] - t_future[0]
                    err2 = tsp_now[1] - t_future[1]
                    reward = compute_reward(err1, err2, reward_scaler)
                    done   = (k >= steps-1)
                    buffer.add_transition(s_t, a_t, next_obs, reward, done)
            else:
                raise ValueError("reward_type must be 1, 2, or 3")

        arduino.Q1(0); arduino.Q2(0)


from collections import deque

def rollout_simulator(policy, buffer, reward_scaler, args):
    from tclab import setup
    lab = setup(connected=False)
    env = lab(synced=False)
    env.Q1(0); env.Q2(0)
    env._T1 = env._T2 = args.ambient

    steps = int(args.max_episode_steps / args.sample_interval)

    # ë™ì¼ TSP ì‚¬ìš©
    Tsp1 = generate_random_tsp1(args.max_episode_steps, args.sample_interval)
    Tsp2 = generate_random_tsp1(args.max_episode_steps, args.sample_interval)

    policy.eval()

    N   = getattr(args, "n_step", 5)             
    obs_q, act_q, tsp_q, temp_q = deque(), deque(), deque(), deque()

    for k in trange(steps, desc="rollout"):
        env.update(t=k * args.sample_interval)
        T1, T2 = env.T1, env.T2

        obs = np.array([T1, T2, Tsp1[k], Tsp2[k]], dtype=np.float32)
        with torch.no_grad():
            # act / online_act / directional_override_act / error_act  / reverse_error_act / hybrid_reverse_act
            act = policy.new_action1(torchify(obs),
                             deterministic=args.deterministic_policy).cpu().numpy()

        Q1, Q2 = float(np.clip(act[0], 0, 100)), float(np.clip(act[1], 0, 100))
        env.Q1(Q1); env.Q2(Q2)

        # 1-step next ê´€ì¸¡
        env.update(t=(k + 1) * args.sample_interval) ####
        next_T1, next_T2 = env.T1, env.T2
        next_obs = np.array(
            [next_T1, next_T2,
             Tsp1[min(k+1, steps-1)], Tsp2[min(k+1, steps-1)]],
            dtype=np.float32
        )

        if args.reward_type == 1:
            err1, err2 = Tsp1[k]                 - T1, \
                         Tsp2[k]                 - T2
            reward = compute_reward(err1, err2, reward_scaler)
            done   = (k == steps - 1)
            buffer.add_transition(obs, [Q1, Q2], next_obs, reward, done)

        elif args.reward_type == 2:
            err1, err2 = Tsp1[min(k+1, steps-1)] - next_T1, \
                         Tsp2[min(k+1, steps-1)] - next_T2
            reward = compute_reward(err1, err2, reward_scaler)
            done   = (k == steps - 1)
            buffer.add_transition(obs, [Q1, Q2], next_obs, reward, done)

        elif args.reward_type == 3:
            obs_q.append(obs);   act_q.append([Q1, Q2])
            tsp_q.append([Tsp1[k], Tsp2[k]])
            temp_q.append([T1, T2])

            if len(obs_q) >= N:
                s_t      = obs_q.popleft()
                a_t      = act_q.popleft()
                tsp_now  = tsp_q.popleft()       # ë§¨ ì• ìš”ì†Œ ì œê±°  / ì™¼ìª½ ë°©í–¥
                t_future = temp_q.pop()          # ë§¨ ë’¤ ìš”ì†Œ ì œê±°  / ì˜¤ë¥¸ìª½ ë°©í–¥ 

                err1 = tsp_now[0] - t_future[0]
                err2 = tsp_now[1] - t_future[1]
                reward = compute_reward(err1, err2, reward_scaler)
                done   = (k >= steps - 1)            # ë§ˆì§€ë§‰ pop íŒë³„
                buffer.add_transition(s_t, a_t, next_obs, reward, done)
        else:
            raise ValueError("reward_type must be 1, 2, or 3")

    env.Q1(0); env.Q2(0)



def online_finetune(args):
    torch.set_num_threads(1)
    
    from replay_buffer import ExperienceBufferManager 
    buffer = ExperienceBufferManager()   
   
    log_dir = Path(args.log_dir)                

    if args.resume:
        print(f"Resuming into existing log dir: {log_dir}")
    else:
        timestamp = datetime.now().strftime("%m-%d-%H%M")
        log_dir = log_dir / args.env_name / f"{args.exp_name}_{timestamp}"

    # â† ì—¬ê¸°ì„œë¶€í„° ê¸°ì¡´ëŒ€ë¡œ
    log = Log(log_dir, vars(args))

    wandb_id = ""
    resume_path = log_dir / "resume_info.json"
    set_seed(args.seed)
    optimizer_factory = build_optimizer_factory(args)

    
    obs_dim, act_dim = 4, 2
    if args.deterministic_policy:
        policy = DeterministicPolicy(obs_dim, act_dim, args.hidden_dim, args.n_hidden)
    else:
        policy = GaussianPolicy(obs_dim, act_dim, args.hidden_dim, args.n_hidden)

    qf = TwinQ(obs_dim, act_dim, args.hidden_dim, args.n_hidden)
    vf = ValueFunction(obs_dim, args.hidden_dim, args.n_hidden)
    iql = ImplicitQLearning(
        qf=qf, vf=vf, policy=policy,
        optimizer_factory=optimizer_factory,
        max_steps=args.n_steps, tau=args.tau, beta=args.beta,
        discount=args.discount, alpha=args.alpha)
    
    resume_path = log_dir / "resume_info.json"
    start_episode = 0

        
    if args.resume and resume_path.exists():
        with open(resume_path) as f:
            resume_data = json.load(f)
        start_episode = resume_data.get("last_episode", 0) + 1
        wandb_id      = resume_data.get("wandb_id", "")
        iql.load_state_dict(torch.load(log_dir / "last.pt"))
        print(f"âœ… Resumed from {start_episode} with last.pt")
    else:
        iql.load_state_dict(torch.load(args.pt_path))
        print(f"ğŸ” Loaded pretrained IQL model from: {args.pt_path}")
        reward_scaler = joblib.load(args.scaler)
    wandb.init(
        project="tclab-project1",
        name=args.exp_name,
        config=vars(args),
        id=wandb_id or wandb.util.generate_id(),
        resume="allow"
    )
    
    if args.init_buffer : 
        buffer.load(args.init_buffer)

    buffer.summary()

    best_total_error = float("inf")
    best_state = None

    for episode in range(args.n_episodes):
        if args.type == "simulator" :
            rollout_simulator(iql.policy, buffer, reward_scaler, args)
        elif args.type == "real" : 
            rollout_tclab(iql.policy, buffer, reward_scaler, args)


        dataset = buffer.to_torch()

        if episode >= args.warmup_episodes : 
            for _ in range(args.update_per_episode):
                batch = sample_batch(dataset, args.batch_size)
                loss_dict = iql.update(**batch)
                iql.policy.epsilon = max(0.05, iql.policy.epsilon * 0.995)
        else : 
            loss_dict = {"policy_loss": 0, "q_loss": 0, "v_loss": 0}

        if episode + 1 == args.warmup_episodes:
            print("âœ… Warm-up ì¢…ë£Œ. í˜„ì¬ ë²„í¼ ìƒíƒœ ìš”ì•½:")
            if hasattr(buffer, 'summary'):
                buffer.summary()
          #  exit()
        if args.type == "simulator" : 
            metrics = evaluate_policy_sim(iql.policy, args)
        elif args.type == "real" :
            metrics = evaluate_policy_tclab(iql.policy, args)

        metrics.update({"episode": episode})
        metrics.update(loss_dict)

        try:
            total_error = (
                metrics["E1"] + metrics["E2"] 
            )
            metrics["total_error"] = total_error
        except KeyError:
            print("âš ï¸ Warning: total_error ê³„ì‚° ì‹¤íŒ¨ (E1, E2, Over, Under ëˆ„ë½)")
            total_error = None

        if total_error is not None and total_error < best_total_error:
            best_total_error = total_error
            best_state = copy.deepcopy(iql.state_dict())
            torch.save(iql.state_dict(), log.dir / 'best.pt')
            print(f"[EP {episode}] âœ… Best model ì €ì¥ë¨ (total_error={total_error:.4f})")

        log.row(metrics)
        wandb.log(metrics)

        if args.save_buffer_path and args.save_buffer_every > 0 \
            and ((episode + 1) % args.save_buffer_every == 0):
            # logs_online_realkit/exp_name_íƒ€ì„ìŠ¤íƒ¬í”„/buffer_ep5.npz í˜•íƒœë¡œ ì €ì¥ë¨
            buf_file = log.dir / f"buffer_ep{episode+1}.npz"
            buffer.save(buf_file)

    if best_state is not None:
        iql.load_state_dict(best_state)
    torch.save(iql.state_dict(), log.dir / 'final_online.pt')
    print(f"ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ ì™„ë£Œ: {log.dir / 'final_online.pt'}")

    if args.save_buffer_path:
        final_buf_path = log.dir / Path(args.save_buffer_path).name
        buffer.save(final_buf_path)
    print(f"replay buffer ì €ì¥ ì™„ë£Œ : {final_buf_path}")

    if args.eval_seeds:
        print("\nğŸ” Extra evaluation with seeds:", args.eval_seeds)

        extra_rows = []
        for s in args.eval_seeds:
            tmp_args = copy.copy(args)
            tmp_args.seed = s
            metrics = eval_policy(iql.policy, tmp_args)

            total_error = (metrics["E1"] + metrics["E2"])
            metrics.update({"total_error": total_error, "seed": s})

            # ì½˜ì†” ì¶œë ¥
            print(f"\n  Seed {s}:")
            print(f"    total_return = {metrics['total_return']:.3f}")
            print(f"    total_error  = {metrics['total_error']:.3f}")

            extra_rows.append(metrics)          # âœ… ë¨¼ì € ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
            log.row({f"extra_s{s}_{k}": v for k, v in metrics.items()})
            wandb.log({f"extra_s{s}_{k}": v for k, v in metrics.items()})

        # --- ëª¨ë“  ì‹œë“œ ìˆ˜ì§‘ í›„ Table í•œ ë²ˆ ìƒì„± ---
     #   tbl = wandb.Table(columns=["seed", "total_error", "total_return"])
     #   for r in extra_rows:
     #       tbl.add_data(r["seed"], r["total_error"], r["total_return"])
     #   wandb.log({"extra_eval_table": tbl})
        tbl = wandb.Table(columns=["seed", "total_error", "total_return"])
        for r in extra_rows:
            tbl.add_data(r["seed"], r["total_error"], r["total_return"])
        wandb.log({"extra_eval_table": tbl})

        avg_return = np.mean([m["total_return"] for m in extra_rows])
        avg_error = np.mean([m["total_error"] for m in extra_rows])

        # ìš”ì²­: avg_error ê°’ì„ total_error ë¡œ ê°„ì£¼
        avg_metrics = {
            "seed": "avg",
            "total_return": avg_return,
            "total_error": avg_error,
        }

        print(
            f"\nğŸ“Š Avg over seeds {args.eval_seeds}: "
            f"total_return = {avg_return:.3f},  total_error = {avg_error:.3f}"
        )

        wandb.log({"extra_avg_total_return": avg_return, "extra_avg_total_error": avg_error})
        log.row(avg_metrics)

        # CSV ì €ì¥
        import csv

        csv_path = log.dir / "extra_eval.csv"
        with open(csv_path, "w", newline="") as f:
            fieldnames = list(extra_rows[0].keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(extra_rows)
            writer.writerow(avg_metrics)
        print(f"âœ… Extra-evaluation results saved to {csv_path}")

    
    wandb.finish()
    log.close()



if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    # sam optimizer / 512 hidden dim 
    # 6ê°œ obs C:/Users/Developer/TCLab/IQL/sam/tclab-mpc-iql/05-07-25_13.28.11_gigk/best.pt
    
    #parser.add_argument('--pt-path', default = "C:/Users/Developer/TCLab/IQL/logs_online_realkit/tclab-online/05-07-25_18.08.30_lzec/ep10.pt")
    # í•´ë‹¹ pt ì •ë³´ : reward type 3 / tau 0.99 / 
    #C:\\Users\\Developer\\TCLab\\IQL\\sam\\tclab-mpc-iql\\offline best\\best.pt
    # C:\Users\Developer\TCLab\IQL\new\tclab-mpc-iql\first_reward\best.pt
    parser.add_argument('--pt-path', default="C:\\Users\\Developer\\TCLab\\IQL\\new\\tclab-mpc-iql\\first_reward\\best.pt")
    # parser.add_argument('--pt-path', default="C:\Users\Developer\TCLabIQL/sam/tclab-mpc-iql/05-14-25_09.05.24_zfsq/best.pt") => sampling interval 1 ì¸ future pt file 
    # ì˜¤í”„ë¼ì¸ í•™ìŠµìœ¼ë¡œ ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ pt íŒŒì¼ , ê²½ë¡œ ì•„ë˜ 
    # C:\Users\Developer\TCLab\Data\reward\first.pkl
    parser.add_argument("--scaler", default="C:/Users/Developer/TCLab/Data/reward/first.pkl")


    parser.add_argument('--exp_name', default="online_ft")
    parser.add_argument('--env-name', default="tclab-online")
    parser.add_argument('--log-dir', default="./logs_online_realkit")
    parser.add_argument('--max-episode-steps', type=int, default=1200)
    parser.add_argument('--sample_interval', type=float, default=5.0)
    #n_episodes=100, update_per_episode=60
    # 1000
# íƒìƒ‰ì´ ìˆëŠ” ì˜¨ë¼ì¸ fine-tune, 20~30 epoch ëª©í‘œ
    parser.add_argument('--n-episodes',         type=int, default=250)   # ì´ episode
    parser.add_argument('--update_per_episode', type=int, default=30)   # episodeë§ˆë‹¤ 60 update
    parser.add_argument('--n-steps',            type=int, default=7500) # 9 000 stepë³´ë‹¤ ì•½ê°„ í¬ê²Œ

    parser.add_argument('--warmup-episodes', type=int, default=0,
                    help='ì´ˆê¸° rolloutë§Œ ìˆ˜í–‰í•˜ê³  ì—…ë°ì´íŠ¸ëŠ” ìƒëµí•  ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--learning-rate', type=float, default=3e-4)
    parser.add_argument('--batch-size', type=int, default=256)
    parser.add_argument('--hidden-dim', type=int, default=256)
    parser.add_argument('--n-hidden', type=int, default=2)
    parser.add_argument('--discount', type=float, default=0.99)
    parser.add_argument('--tau', type=float, default=0.99)
    parser.add_argument('--beta', type=float, default=3.0)
    parser.add_argument('--alpha', type=float, default=0.005)
    parser.add_argument('--seed', type=int, default=3)
    parser.add_argument('--ambient', type=float, default=29.0)
    parser.add_argument('--stochastic-policy', action='store_false', dest='deterministic_policy')
    parser.add_argument("--sam", action="store_true",
                        help="SAM(Sharpnessâ€‘Aware Minimization) ì‚¬ìš© ì—¬ë¶€")
    parser.add_argument("--sam-rho", type=float, default=0.05,
                        help="SAM perturbation ë°˜ê²½ Ï")

# C:\Users\Developer\TCLab\Data\reward\first.npz
    parser.add_argument("--init-buffer", default="C:/Users/Developer/TCLab/Data/reward/first.npz",
                         help="ì‹œì‘ ì‹œ ë¶ˆëŸ¬ì˜¬ .npz ë²„í¼ ê²½ë¡œ")
    parser.add_argument("--type", default="simulator", help="rollout ì¢…ë¥˜ ì„¤ì • (simulator / real)")
    parser.add_argument("--save-buffer-path", default="./saved_buffer.npz",
                    help="ëˆ„ì  rollout ì„ ì €ì¥í•  .npz ê²½ë¡œ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)")
    parser.add_argument("--save-buffer-every", type=int, default=2,
                        help="N ì—í”¼ì†Œë“œë§ˆë‹¤ ë²„í¼ë¥¼ ì €ì¥ (0ì´ë©´ ë§ˆì§€ë§‰ì—ë§Œ ì €ì¥)")
    parser.add_argument("--resume", action="store_true", help="ì´ì „ í•™ìŠµ ì´ì–´ì„œ ì¬ê°œí• ì§€ ì—¬ë¶€")
    parser.add_argument("--reward_type", type=int, default=1)
    parser.add_argument(
        "--eval-seeds",
        nargs="*",
        type=int,
        default=[0, 1, 2],
        help="ì¶”ê°€ í‰ê°€ìš© random seed ëª©ë¡ (ì˜ˆ: --eval-seeds 0 1 2 )",
    )


    args = parser.parse_args()
    print(args.scaler)
    online_finetune(args)



